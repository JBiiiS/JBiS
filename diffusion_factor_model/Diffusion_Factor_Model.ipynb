{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96699065",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U  finance-datareader # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23846849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # pandas 명시적 import 필요\n",
    "import FinanceDataReader as fdr # type: ignore\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 우리가 만든 모듈들 임포트\n",
    "# (data_process.py에 clean_price_data, log_data가 있다고 가정)\n",
    "from base.data_process import * \n",
    "from diffusion_config import DiffusionConfig\n",
    "from diffusion_factor_model import Encoder, Decoder\n",
    "from time_dynamics import TemporalDynamics\n",
    "from base.noise import GaussianDiffusion\n",
    "from base.pca import PCA\n",
    "from base.loss_functions import MSE_Loss\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. 설정 및 장치 초기화\n",
    "# ---------------------------------------------------------\n",
    "config = DiffusionConfig()\n",
    "\n",
    "# [안전장치] Config에 device가 없으면 여기서 직접 설정\n",
    "if hasattr(config, 'device'):\n",
    "    device = config.device\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device set to: {device}\")\n",
    "\n",
    "\n",
    "config_dict = config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227dc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. 데이터 다운로드 설정\n",
    "# ---------------------------------------------------------\n",
    "# 일단 목표 개수는 config에서 가져오되, 나중에 업데이트할 예정\n",
    "TARGET_N_STOCKS = config.num_assets \n",
    "START_DATE = '2015-01-01'\n",
    "END_DATE = '2025-12-31'\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. S&P 500 종목 리스트 가져오기\n",
    "# ---------------------------------------------------------\n",
    "print(\"S&P 500 종목 리스트를 불러오는 중...\")\n",
    "try:\n",
    "    sp500 = fdr.StockListing('S&P500')\n",
    "    all_tickers = sp500['Symbol'].tolist()\n",
    "    print(f\"총 {len(all_tickers)}개의 종목을 찾았습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"S&P500 리스트 로드 실패 (인터넷 연결 확인 필요): {e}\")\n",
    "    # 테스트용 더미 리스트 (실패 시 비상용)\n",
    "    all_tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'NVDA', 'TSLA', 'META', 'BRK-B', 'LLY', 'V']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. 랜덤하게 n개 뽑기\n",
    "# ---------------------------------------------------------\n",
    "if TARGET_N_STOCKS > len(all_tickers):\n",
    "    selected_tickers = all_tickers\n",
    "else:\n",
    "    selected_tickers = random.sample(all_tickers, TARGET_N_STOCKS)\n",
    "\n",
    "print(f\"다운로드 시도할 종목 수: {len(selected_tickers)}\")\n",
    "print(f\"예시: {selected_tickers[:5]} ...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. 데이터 다운로드 및 병합\n",
    "# ---------------------------------------------------------\n",
    "close_data_dict = {}\n",
    "\n",
    "print(\"데이터 다운로드 시작...\")\n",
    "for ticker in tqdm(selected_tickers):\n",
    "    try:\n",
    "        df = fdr.DataReader(ticker, START_DATE, END_DATE)\n",
    "        \n",
    "        # 데이터가 있고, 결측치가 너무 많지 않은 경우에만 추가\n",
    "        if not df.empty and 'Close' in df.columns:\n",
    "            close_data_dict[ticker] = df['Close']\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {ticker}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. DataFrame 생성 및 전처리\n",
    "# ---------------------------------------------------------\n",
    "price_df = pd.DataFrame(close_data_dict)\n",
    "\n",
    "# [중요] 전처리 함수들이 정의되어 있어야 합니다.\n",
    "# 만약 import * 로 안 불러와졌다면 여기서 정의하거나 data_process.py 확인 필요\n",
    "price_df = clean_price_data(price_df)\n",
    "\n",
    "# 로그 수익률 변환\n",
    "ln_price, ln_price_diff = log_data(price_df)\n",
    "\n",
    "# 추가 정제\n",
    "ln_price = clean_price_data(ln_price)\n",
    "ln_price_diff = clean_price_data(ln_price_diff)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. [CRITICAL] Config 업데이트 (실제 데이터 개수에 맞춤)\n",
    "# ---------------------------------------------------------\n",
    "actual_n_stocks = ln_price_diff.shape[1]\n",
    "\n",
    "if actual_n_stocks != config.num_assets:\n",
    "    print(f\"\\n[주의] 요청한 종목 수({config.num_assets})와 다운로드 성공한 수({actual_n_stocks})가 다릅니다.\")\n",
    "    print(f\"Config.num_assets를 {actual_n_stocks}로 자동 업데이트합니다.\")\n",
    "    config.num_assets = actual_n_stocks\n",
    "    \n",
    "    # [Tip] 만약 num_factors가 종목 수보다 크면 에러 나므로 체크\n",
    "    if config.num_factors > actual_n_stocks:\n",
    "        print(f\"Warning: 팩터 개수({config.num_factors})가 종목 수보다 많습니다. 절반으로 조정합니다.\")\n",
    "        config.num_factors = actual_n_stocks // 2\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. 최종 확인\n",
    "# ---------------------------------------------------------\n",
    "print(\"-\" * 50)\n",
    "print(f\"[완료] 데이터프레임 생성됨.\")\n",
    "print(f\"Price Data Shape: {ln_price.shape} (Row: 날짜, Col: 종목)\")\n",
    "print(f\"Diff Data Shape: {ln_price_diff.shape} (Input Data)\")\n",
    "print(f\"Config Num Assets: {config.num_assets}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 결과 확인\n",
    "ln_price_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d223a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. 데이터 전처리 (To_TensorSet 실행)\n",
    "# ---------------------------------------------------------\n",
    "print(\"데이터 텐서 변환 및 분할 시작...\")\n",
    "\n",
    "# (1) 프로세서 인스턴스 생성 (Config 필요)\n",
    "tensor_processor = To_TensorSet(config)\n",
    "\n",
    "# (2) process 메서드 호출 (DataFrame -> Dictionary)\n",
    "# diffusion_data_dict['train']['FULL']에 (N, d, T) 형태의 텐서가 들어있습니다.\n",
    "diffusion_data_dict = tensor_processor.process(ln_price_diff)\n",
    "\n",
    "print(\"텐서 변환 완료.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96235a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. PCA 수행 (Train Set 전체 데이터 사용)\n",
    "# ---------------------------------------------------------\n",
    "# PCA는 학습 데이터 전체의 분포를 봐야 하므로 배치로 나누기 전의 'FULL' 데이터를 사용합니다.\n",
    "train_full_data = diffusion_data_dict['train']['FULL']\n",
    "\n",
    "# GPU 사용 가능하다면 GPU로 이동 (연산 가속)\n",
    "train_full_data = train_full_data.to(device)\n",
    "\n",
    "print(f\"\\nPCA 분석 시작... (Input Shape: {train_full_data.shape})\")\n",
    "\n",
    "# PCA 함수 호출\n",
    "# k = config.num_factors (예: 10개 팩터)\n",
    "cov_full, eig_vals, diag_variances = PCA(train_full_data, k=config.num_factors)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. 결과 저장 및 확인\n",
    "# ---------------------------------------------------------\n",
    "# [중요] diag_variances는 나중에 Loss Function에서 가중치로 계속 쓰이므로\n",
    "# 미리 device에 올려두고 변수로 저장해둡니다.\n",
    "diag_variances = diag_variances.to(device)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"PCA 완료. 추출된 잔차 분산(Variance) 개수: {len(diag_variances)}\")\n",
    "print(f\"상위 5개 고유값(Eigenvalues): {eig_vals[:5].cpu().numpy()}\") # 확인용 출력\n",
    "print(f\"첫 5개 종목의 잔차 분산: {diag_variances[:5].cpu().numpy()}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 시각화 (선택 사항: 고유값 감소 그래프 Scree Plot)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(eig_vals.cpu().numpy(), marker='o')\n",
    "plt.title(\"Scree Plot (Eigenvalues)\")\n",
    "plt.xlabel(\"Factor Index\")\n",
    "plt.ylabel(\"Eigenvalue Size\")\n",
    "plt.grid(True)\n",
    "plt.show() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b33065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# 1. 모델 및 설정 초기화 (Setup)\n",
    "# ---------------------------------------------------------\n",
    "print(\"모델 초기화 및 GPU 설정 중...\")\n",
    "\n",
    "# (1) 설정 및 장치\n",
    "# config는 앞서 정의한 DiffusionConfig 객체여야 합니다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Device: {device}\")\n",
    "\n",
    "# (2) 모델 인스턴스 생성\n",
    "# Encoder: (Batch, D, T) -> (Batch, k, T)\n",
    "encoder = Encoder(input_dim=config.num_assets, \n",
    "                  factor_dim=config.num_factors, \n",
    "                  num_layers=2).to(device)\n",
    "\n",
    "# Noise Generator: (Batch, k, T) -> (Batch, k, T)\n",
    "# (파라미터 학습 X, 버퍼만 있음)\n",
    "diffusion = GaussianDiffusion(config).to(device)\n",
    "\n",
    "# Time Dynamics: (Batch, k, T) + t -> (Batch, k, T)\n",
    "dynamics = TemporalDynamics(factor_dim=config.num_factors, \n",
    "                            time_emb_dim=config.hidden_dim, \n",
    "                            num_layers=4).to(device)\n",
    "\n",
    "# Decoder: (Batch, k, T) -> (Batch, D, T)\n",
    "decoder = Decoder(factor_dim=config.num_factors, \n",
    "                  output_dim=config.num_assets, \n",
    "                  num_layers=2).to(device)\n",
    "\n",
    "# (3) Optimizer 설정\n",
    "\n",
    "all_parameters = list(itertools.chain(\n",
    "    encoder.parameters(),\n",
    "    dynamics.parameters(),\n",
    "    decoder.parameters()\n",
    "))\n",
    "\n",
    "optimizer = optim.AdamW(all_parameters, lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# (4) Loss Function 설정\n",
    "# use_cov_weight=True -> PCA 잔차 분산 사용\n",
    "loss_fn_R = MSE_Loss(use_cov_weight=True, prepared_var_mtx=True).to(device)\n",
    "loss_fn_F = MSE_Loss(use_cov_weight=False).to(device) # 팩터는 단순 MSE\n",
    "\n",
    "# PCA 결과인 잔차 분산을 GPU로 이동 (Loss 계산용)\n",
    "diag_variances = diag_variances.to(device)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 학습 루프 (Training Loop)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n학습 시작...\")\n",
    "\n",
    "# 데이터 로더 (List of Tensors from To_TensorSet)\n",
    "train_loader = diffusion_data_dict['train']['BDT'] # (Batch, D, T) 형태\n",
    "\n",
    "EPOCHS = config.num_epochs\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss_total = 0.0\n",
    "    epoch_loss_r = 0.0\n",
    "    epoch_loss_f = 0.0\n",
    "    \n",
    "    # --- Batch Loop ---\n",
    "    for batch_idx, real_data in enumerate(train_loader):\n",
    "        # 1. 데이터 준비\n",
    "        # real_data: (Batch, D, T) -> GPU 이동\n",
    "        x_0 = real_data.to(device)\n",
    "        batch_size = x_0.size(0)\n",
    "        \n",
    "        # Optimizer 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # [Step 1] Encoder: Clean Data -> Clean Factor\n",
    "        # -------------------------------------------------\n",
    "        f_0 = encoder(x_0) # (B, k, T)\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # [Step 2] Diffusion: Clean Factor -> Noisy Factor\n",
    "        # -------------------------------------------------\n",
    "        # f_0에 노이즈 주입 (학습용 문제 출제)\n",
    "        f_t, noise_z, t = diffusion(f_0) \n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # [Step 3] Dynamics: Noisy Factor -> Predicted Factor\n",
    "        # -------------------------------------------------\n",
    "        # \"노이즈 낀 f_t를 보고 원본 f_0를 맞춰봐\"\n",
    "        f_0_hat = dynamics(f_t, t)\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # [Step 4] Decoder: Predicted Factor -> Reconstructed Data\n",
    "        # -------------------------------------------------\n",
    "        # \"복원된 팩터로 자산 가격 다시 그려봐\"\n",
    "        r_0_hat = decoder(f_0_hat)\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # [Step 5] Loss Calculation\n",
    "        # -------------------------------------------------\n",
    "        \n",
    "        # (A) Reconstruction Loss (자산 복원)\n",
    "        # PCA 잔차 분산(diag_variances)을 가중치로 사용\n",
    "        loss_R = loss_fn_R(r_0_hat, x_0, var_mtx=diag_variances)\n",
    "        \n",
    "        # (B) Latent Consistency Loss (팩터 일관성)\n",
    "        # Encoder가 만든 정답(f_0) vs Dynamics가 예측한 값(f_0_hat)\n",
    "        loss_F = loss_fn_F(f_0_hat, f_0)\n",
    "        \n",
    "        # (C) Total Loss\n",
    "        # lambda_f = 0.1 (팩터 로스 반영 비율, 조절 가능)\n",
    "        lambda_f = 0.1\n",
    "        total_loss = loss_R + (lambda_f * loss_F)\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # [Step 6] Backpropagation\n",
    "        # -------------------------------------------------\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (학습 안정성)\n",
    "        torch.nn.utils.clip_grad_norm_(all_parameters, max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 로그 기록\n",
    "        epoch_loss_total += total_loss.item()\n",
    "        epoch_loss_r += loss_R.item()\n",
    "        epoch_loss_f += loss_F.item()\n",
    "\n",
    "    # --- Epoch End ---\n",
    "    # 평균 Loss 계산\n",
    "    avg_total = epoch_loss_total / len(train_loader)\n",
    "    avg_r = epoch_loss_r / len(train_loader)\n",
    "    avg_f = epoch_loss_f / len(train_loader)\n",
    "    loss_history.append(avg_total)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Total: {avg_total:.6f} | Recon(R): {avg_r:.6f} | Latent(F): {avg_f:.6f}\")\n",
    "\n",
    "print(\"학습 완료.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. 학습 결과 시각화 (Loss Curve)\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Total Loss')\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
